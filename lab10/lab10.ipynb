{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from collections import Counter\n",
    "from typing import Callable, List, Set\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game():\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    \n",
    "    while available:\n",
    "        x = choice(list(available))\n",
    "        state.x.add(x)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(x)\n",
    "        if win(state.x) or not available:\n",
    "            break\n",
    "\n",
    "        o = choice(list(available))\n",
    "        state.o.add(o)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(o)\n",
    "        if win(state.o):\n",
    "            break\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bac579e22de4407b3acaef430984ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_dictionary = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "epsilon = 0.001\n",
    "\n",
    "for steps in tqdm(range(500_000)):\n",
    "    trajectory = random_game()\n",
    "    final_reward = state_value(trajectory[-1])\n",
    "    for state in trajectory:\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "        hit_state[hashable_state] += 1\n",
    "        value_dictionary[hashable_state] = value_dictionary[\n",
    "            hashable_state\n",
    "        ] + epsilon * (final_reward - value_dictionary[hashable_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset({4, 5, 6}), frozenset({1, 2})), 0.9999999999999445),\n",
       " ((frozenset({5, 6}), frozenset({1, 2})), 0.9930526291561056),\n",
       " ((frozenset({5, 6}), frozenset({1})), 0.9872972345061332),\n",
       " ((frozenset({1, 6, 7, 8, 9}), frozenset({2, 3, 4, 5})), 0.9223293518739043),\n",
       " ((frozenset({1, 4, 5, 6, 7}), frozenset({2, 3, 8, 9})), 0.9216268080073997),\n",
       " ((frozenset({1, 3, 5, 6, 9}), frozenset({2, 4, 7, 8})), 0.9195613374019573),\n",
       " ((frozenset({1, 2, 3, 5, 9}), frozenset({4, 6, 7, 8})), 0.9186711800754285),\n",
       " ((frozenset({4, 5, 6, 8, 9}), frozenset({1, 2, 3, 7})), 0.9185897698452737),\n",
       " ((frozenset({1, 2, 3, 4, 9}), frozenset({5, 6, 7, 8})), 0.9180176118921533),\n",
       " ((frozenset({2, 4, 5, 7, 8}), frozenset({1, 3, 6, 9})), 0.9179355474395928)]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(value_dictionary.items(), key=lambda e: e[1], reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for evaluating\n",
    "\n",
    "Before I start initialize refinforment learning policy for playing the game, I define a metric that measures the wining rate while playing with a random player. The method considers both possibilities of starting the game (whether which player plays first). Furthermore, I try to create a class for selecting the next action among all available choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    def __init__(self):\n",
    "        self.typ = 'ranodm'\n",
    "        \n",
    "    def get(self, state: State, available: Set[int]) -> int:\n",
    "        x = choice(list(available))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self, x_policy, o_policy):\n",
    "        self.x_policy = x_policy\n",
    "        self.o_policy = o_policy\n",
    "        self._empty()\n",
    "        \n",
    "    def _empty(self):\n",
    "        self.trajectory = list()\n",
    "        self.state = State(set(), set())\n",
    "        self.available = set(range(1, 9+1))\n",
    "        \n",
    "    def _get_stats(self, reward_list):\n",
    "        return Counter(reward_list)\n",
    "        \n",
    "        \n",
    "    def play(self, switch):\n",
    "        while self.available:\n",
    "            if not switch:\n",
    "                x = self.x_policy.get(self.state, self.available)\n",
    "            else:\n",
    "                x = self.o_policy.get(self.state, self.available)\n",
    "                \n",
    "            self.state.x.add(x)\n",
    "            self.trajectory.append(deepcopy(self.state))\n",
    "            self.available.remove(x)\n",
    "            if win(self.state.x) or not self.available:\n",
    "                break\n",
    "            \n",
    "            if not switch:\n",
    "                o = self.o_policy.get(self.state, self.available)\n",
    "            else:\n",
    "                o = self.x_policy.get(self.state, self.available)\n",
    "                \n",
    "            self.state.o.add(o)\n",
    "            self.trajectory.append(deepcopy(self.state))\n",
    "            self.available.remove(o)\n",
    "            if win(self.state.o):\n",
    "                break\n",
    "            \n",
    "        traj = deepcopy(self.trajectory)\n",
    "        self._empty()\n",
    "        return traj\n",
    "    \n",
    "    \n",
    "    def evaluate(self, n_games):\n",
    "        rewards = []  \n",
    "        for _ in tqdm(range(n_games)):\n",
    "            tr = self.play(switch=False)\n",
    "            reward = state_value(tr[-1])\n",
    "            rewards.append(reward)\n",
    "        stats = self._get_stats(rewards)\n",
    "\n",
    "        \n",
    "        print(f'total number of plays: {n_games}\\n')\n",
    "        print(f'winning accuracy of {self.x_policy.typ}: {stats[1]/n_games}\\n')\n",
    "        print(40*'_')\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of plays: 1000\n",
      "\n",
      "winning accuracy of ranodm: 0.589\n",
      "\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_policy = RandomPolicy()\n",
    "o_policy = RandomPolicy()\n",
    "\n",
    "eval = Evaluation(x_policy, o_policy)\n",
    "eval.evaluate(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Policy Conclusion\n",
    "In random policy we get na average of 50 percent wins which we expected to get. Now we try to optimize the next action selection by value iteration. In the labratory of the course, we played the game for many times to obtain \"value_dictionary\". Now we try to find optimum policy $\\pi^*$ that selects best action based on best values\n",
    "\n",
    "\n",
    "Value iteration is an algorithm used in reinforcement learning to find the optimal value function and policy for a Markov decision process (MDP). The algorithm iteratively updates the value function for each state until convergence.\n",
    "\n",
    "### Update Equation\n",
    "$V_{k+1}(s) = \\max_a \\left( R(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V_k(s') \\right)$\n",
    "\n",
    "\n",
    "### Optimal Policy\n",
    "$ \\pi^*(s) = \\arg\\max_a \\left( R(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V^*(s') \\right) $\n",
    "\n",
    "\n",
    "Value iteration converges to the optimal values and policy, making it a key algorithm in solving MDPs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimumPolicy:\n",
    "    def __init__(self,):\n",
    "        self.vd = defaultdict(float)\n",
    "        self.typ = 'optimum'\n",
    "    \n",
    "    def set_vd(self, value_dictionary):\n",
    "        self.vd = value_dictionary\n",
    "        \n",
    "    def get_vd(self):\n",
    "        return self.vd\n",
    "        \n",
    "    def get(self, state: State, available: Set[int]) -> int:\n",
    "        temp_val = {}\n",
    "        for mov in available:\n",
    "            new_state = deepcopy(state)\n",
    "            new_state.x.add(mov)\n",
    "            temp_val[mov] = self.vd[(frozenset(new_state.x), frozenset(new_state.o))]\n",
    "        \n",
    "        max_value = max(temp_val, key=temp_val.get)\n",
    "        return max_value\n",
    "    \n",
    "    def iteration(self, n_steps):\n",
    "        for steps in tqdm(range(n_steps)):\n",
    "            trajectory = self._play()\n",
    "            final_reward = state_value(trajectory[-1])\n",
    "            for state in trajectory:\n",
    "                hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "                self.vd[hashable_state] = self.vd[\n",
    "                    hashable_state\n",
    "                ] + epsilon * (final_reward - self.vd[hashable_state])\n",
    "\n",
    "                \n",
    "\n",
    "    def _play(self):\n",
    "        trajectory = list()\n",
    "        state = State(set(), set())\n",
    "        available = set(range(1, 9+1))\n",
    "\n",
    "        while available:\n",
    "            x = self.get(state, available)\n",
    "            state.x.add(x)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(x)\n",
    "            if win(state.x) or not available:\n",
    "                break\n",
    "\n",
    "            o = self.get(state, available)\n",
    "            state.o.add(o)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(o)\n",
    "            if win(state.o):\n",
    "                break\n",
    "        return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing optimum policy\n",
    "Now we try to evaluate the $\\pi^*$ policy against random policy. We can also use value iteration to update the value dictionary to have better winning accuracy. All the rules implemented based on the assumption that our agent plays first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678e0fcb8d484db5b285c5d68783064a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of plays: 10000\n",
      "\n",
      "winning accuracy of optimum: 0.9893\n",
      "\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_policy = OptimumPolicy()\n",
    "x_policy.set_vd(value_dictionary)\n",
    "o_policy = RandomPolicy()\n",
    "\n",
    "\n",
    "eval = Evaluation(x_policy, o_policy)\n",
    "eval.evaluate(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7556db592e824fe7b0b289aa77b72631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_policy.iteration(50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadc53b56f614f8d9dcfe017bc8caf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of plays: 10000\n",
      "\n",
      "winning accuracy of optimum: 0.9904\n",
      "\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "eval = Evaluation(x_policy, o_policy)\n",
    "eval.evaluate(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd = x_policy.get_vd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "We can see now after 500_000 iteration, we got better optimum policy. Now we try to implement QLerarning technique for further investigation. First let's discuss about Q-values. \n",
    "\n",
    "In reinforcement learning, the Q-value $Q(s, a)$ represents the expected cumulative reward when an agent takes action $a$ in state $s$. It is defined by the Bellman equation:\n",
    "\n",
    "$Q(s, a) = R(s, a) + \\gamma \\max_{a'} Q(s', a')$\n",
    "\n",
    "Here, $R(s, a)$ is the immediate reward, $s$ is the next state, \\(\\gamma\\) is the discount factor, and $\\max_{a'} Q(s', a')$ is the maximum Q-value in the next state. Q-values are crucial in algorithms like Q-learning for optimizing an agent's policy. Based on the \"values_dictionary\" which we obtained during previous iteration, we consider immediate values of each $R(s,a)$ based on new learning of the values wrt the action made in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_random_game():\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    \n",
    "    while available:\n",
    "        x = choice(list(available))\n",
    "        state.x.add(x)\n",
    "        trajectory.append((deepcopy(state), x))\n",
    "        available.remove(x)\n",
    "        if win(state.x) or not available:\n",
    "            break\n",
    "\n",
    "        o = choice(list(available))\n",
    "        state.o.add(o)\n",
    "        trajectory.append((deepcopy(state), x))\n",
    "        available.remove(o)\n",
    "        if win(state.o):\n",
    "            break\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4e84dbaae24a5fb34f4799e27cd304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_table = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "epsilon = 0.001\n",
    "\n",
    "for steps in tqdm(range(1_000_000)):\n",
    "    trajectory = new_random_game()\n",
    "    final_reward = state_value(trajectory[-1][0])\n",
    "    for state, action in trajectory:\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o), action)\n",
    "        hit_state[hashable_state] += 1\n",
    "        q_table[hashable_state] = q_table[\n",
    "            hashable_state\n",
    "        ] + epsilon * (final_reward - q_table[hashable_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset({1, 2, 4, 5, 8}), frozenset({3, 6, 7, 9}), 2),\n",
       "  0.8262043479500509),\n",
       " ((frozenset({1, 2, 3, 5, 9}), frozenset({4, 6, 7, 8}), 9),\n",
       "  0.8117218817177256),\n",
       " ((frozenset({3, 5, 6, 7, 9}), frozenset({1, 2, 4, 8}), 5),\n",
       "  0.8113447598927512),\n",
       " ((frozenset({1, 4, 5, 6, 7}), frozenset({2, 3, 8, 9}), 5),\n",
       "  0.8111559158085598),\n",
       " ((frozenset({1, 2, 4, 5, 8}), frozenset({3, 6, 7, 9}), 8),\n",
       "  0.8107776603516027),\n",
       " ((frozenset({2, 4, 7, 8, 9}), frozenset({1, 3, 5, 6}), 9), 0.810588248600203),\n",
       " ((frozenset({2, 3, 5, 8, 9}), frozenset({1, 4, 6, 7}), 2),\n",
       "  0.8102088561035539),\n",
       " ((frozenset({1, 2, 3, 4, 8}), frozenset({5, 6, 7, 9}), 8),\n",
       "  0.8083004642615419),\n",
       " ((frozenset({2, 6, 7, 8, 9}), frozenset({1, 3, 4, 5}), 2), 0.8079164893237),\n",
       " ((frozenset({2, 3, 5, 8, 9}), frozenset({1, 4, 6, 7}), 5),\n",
       "  0.8073390843668866)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(q_table.items(), key=lambda e: e[1], reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPolicy:\n",
    "    def __init__(self,):\n",
    "        self.q_table = defaultdict(float)\n",
    "        self.typ = 'Q'\n",
    "    \n",
    "    def set_q_table(self, q_table):\n",
    "        self.q_table = q_table\n",
    "        \n",
    "    def get_q_table(self):\n",
    "        return self.q_table\n",
    "        \n",
    "    def get(self, state: State, available: Set[int]) -> int:\n",
    "        temp_val = {}\n",
    "        cp_state = deepcopy(state)\n",
    "        for mov in available:\n",
    "            temp_val[mov] = self.Q(cp_state, mov)\n",
    "        max_value = max(temp_val, key=temp_val.get)\n",
    "        return max_value\n",
    "    \n",
    "    def iteration(self, n_steps):\n",
    "        for steps in tqdm(range(n_steps)):\n",
    "            trajectory = self._play()\n",
    "            final_reward = state_value(trajectory[-1][0])\n",
    "            for state, action in trajectory:\n",
    "                hashable_state = (frozenset(state.x), frozenset(state.o), action)\n",
    "                self.q_table[hashable_state] = self.q_table[\n",
    "                    hashable_state\n",
    "                ] + epsilon * (final_reward - self.q_table[hashable_state])\n",
    "\n",
    "                \n",
    "\n",
    "    def _play(self):\n",
    "        trajectory = list()\n",
    "        state = State(set(), set())\n",
    "        available = set(range(1, 9+1))\n",
    "\n",
    "        while available:\n",
    "            x = self.get(state, available)\n",
    "            state.x.add(x)\n",
    "            trajectory.append((deepcopy(state), x))\n",
    "            available.remove(x)\n",
    "            if win(state.x) or not available:\n",
    "                break\n",
    "\n",
    "            o = self.get(state, available)\n",
    "            state.o.add(o)\n",
    "            trajectory.append((deepcopy(state), x))\n",
    "            available.remove(o)\n",
    "            if win(state.o):\n",
    "                break\n",
    "        return trajectory\n",
    "    \n",
    "    \n",
    "    def R(self, state: State, action: int) -> float:\n",
    "        return self.q_table[(frozenset(state.x), frozenset(state.o), action)]\n",
    "\n",
    "    def Q(self, state: State, action: int, gamma: float = 0.9) -> float:\n",
    "\n",
    "        s = deepcopy(state)\n",
    "        s.x.add(action)\n",
    "        current_r = self.R(s, action)\n",
    "        taken = s.x.union(s.o)\n",
    "        available = set(MAGIC) - taken\n",
    "        temp_val = []\n",
    "        if available:\n",
    "            for mov in available:\n",
    "                next_state = deepcopy(s)\n",
    "                next_available = deepcopy(available)\n",
    "                next_state.o.add(mov)\n",
    "                next_available = next_available - {mov}\n",
    "\n",
    "                if next_available:\n",
    "                    for next_mov in next_available:\n",
    "                        st_cpy = deepcopy(next_state)\n",
    "                        n_val = self.Q(st_cpy, next_mov)\n",
    "                        temp_val.append(n_val)\n",
    "\n",
    "\n",
    "        if len(temp_val)==0:\n",
    "            return current_r\n",
    "        else:\n",
    "            max_value = max(temp_val)\n",
    "            return current_r + gamma*max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e9c52c28d04f9f81af204ef4c22a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of plays: 5\n",
      "\n",
      "winning accuracy of Q: 1.0\n",
      "\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "o_policy = RandomPolicy()\n",
    "x_policy = QPolicy()\n",
    "x_policy.set_q_table(q_table)\n",
    "\n",
    "eval = Evaluation(x_policy, o_policy)\n",
    "eval.evaluate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8757e58870e74dd5a1209a9fd3f1b4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_policy = QPolicy()\n",
    "x_policy.set_q_table(q_table)\n",
    "x_policy.iteration(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                if c==2:\n",
    "                    print(f'|{color.BOLD}X{color.END}|', end='')\n",
    "                else:\n",
    "                    print(f'|{color.BOLD}X{color.END}', end='')\n",
    "                    \n",
    "            elif MAGIC[i] in pos.o:\n",
    "                if c==2:\n",
    "                    print(f'|{color.BOLD}O{color.END}|', end='')\n",
    "                else:\n",
    "                    print(f'|{color.BOLD}O{color.END}', end='')\n",
    "            else:\n",
    "                if c==2:\n",
    "                    print(f'|{MAGIC[i]}|', end='')\n",
    "                else:\n",
    "                    print(f'|{MAGIC[i]}', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(available):\n",
    "    x = int(input(f\"Enter your move from {available}:  \"))\n",
    "    if x not in available:\n",
    "        print(f'{x} is not available')\n",
    "        x = get_input(available)\n",
    "    return x\n",
    "        \n",
    "def experiment(policy):\n",
    "    \n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "\n",
    "    while available:\n",
    "        print(f'\\ncurrent board with available positions in\\n')\n",
    "        print_board(state)\n",
    "        x = get_input(available)\n",
    "        state.x.add(x)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(x)\n",
    "        if win(state.x):\n",
    "            print(f\"\\nCongrats! you win the game.\\n\")\n",
    "            print(30*'_')\n",
    "            break\n",
    "        if not available:\n",
    "            print(f\"\\nit's draw.\\n\")\n",
    "            print(30*'_')\n",
    "            break\n",
    "            \n",
    "        o = policy.get(state, available)\n",
    "        state.o.add(o)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(o)\n",
    "        if win(state.o):\n",
    "            print(f\"\\nYou messed up unfortunately!\\n\")\n",
    "            print(30*'_')\n",
    "            break\n",
    "            \n",
    "        print(30*'_')\n",
    "    print('\\n')\n",
    "    \n",
    "    play_again = str(input(\"Do you want to play again? (y/n) :\"))\n",
    "    if play_again==\"y\":\n",
    "        experiment(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current board with available positions in\n",
      "\n",
      "|2|7|6|\n",
      "|9|5|1|\n",
      "|4|3|8|\n",
      "\n",
      "Enter your move from {1, 2, 3, 4, 5, 6, 7, 8, 9}:  1\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mO\u001b[0m|7|6|\n",
      "|9|5|\u001b[1mX\u001b[0m|\n",
      "|4|3|8|\n",
      "\n",
      "Enter your move from {3, 4, 5, 6, 7, 8, 9}:  5\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mO\u001b[0m|7|6|\n",
      "|9|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|\n",
      "|4|\u001b[1mO\u001b[0m|8|\n",
      "\n",
      "Enter your move from {4, 6, 7, 8, 9}:  9\n",
      "\n",
      "Congrats! you win the game.\n",
      "\n",
      "______________________________\n",
      "\n",
      "\n",
      "Do you want to play again? (y/n) :y\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|2|7|6|\n",
      "|9|5|1|\n",
      "|4|3|8|\n",
      "\n",
      "Enter your move from {1, 2, 3, 4, 5, 6, 7, 8, 9}:  2\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mX\u001b[0m|7|6|\n",
      "|9|5|\u001b[1mO\u001b[0m|\n",
      "|4|3|8|\n",
      "\n",
      "Enter your move from {3, 4, 5, 6, 7, 8, 9}:  7\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|6|\n",
      "|9|5|\u001b[1mO\u001b[0m|\n",
      "|4|\u001b[1mO\u001b[0m|8|\n",
      "\n",
      "Enter your move from {4, 5, 6, 8, 9}:  9\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|6|\n",
      "|\u001b[1mX\u001b[0m|5|\u001b[1mO\u001b[0m|\n",
      "|\u001b[1mO\u001b[0m|\u001b[1mO\u001b[0m|8|\n",
      "\n",
      "Enter your move from {5, 6, 8}:  5\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|\u001b[1mO\u001b[0m|\n",
      "|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|\u001b[1mO\u001b[0m|\n",
      "|\u001b[1mO\u001b[0m|\u001b[1mO\u001b[0m|8|\n",
      "\n",
      "Enter your move from {8}:  8\n",
      "\n",
      "Congrats! you win the game.\n",
      "\n",
      "______________________________\n",
      "\n",
      "\n",
      "Do you want to play again? (y/n) :n\n"
     ]
    }
   ],
   "source": [
    "x_policy = OptimumPolicy()\n",
    "x_policy.set_vd(value_dictionary)\n",
    "experiment(x_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current board with available positions in\n",
      "\n",
      "|2|7|6|\n",
      "|9|5|1|\n",
      "|4|3|8|\n",
      "\n",
      "Enter your move from {1, 2, 3, 4, 5, 6, 7, 8, 9}:  2\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mX\u001b[0m|7|6|\n",
      "|9|5|\u001b[1mO\u001b[0m|\n",
      "|4|3|8|\n",
      "\n",
      "Enter your move from {3, 4, 5, 6, 7, 8, 9}:  7\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|6|\n",
      "|9|5|\u001b[1mO\u001b[0m|\n",
      "|4|\u001b[1mO\u001b[0m|8|\n",
      "\n",
      "Enter your move from {4, 5, 6, 8, 9}:  9\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|6|\n",
      "|\u001b[1mX\u001b[0m|5|\u001b[1mO\u001b[0m|\n",
      "|\u001b[1mO\u001b[0m|\u001b[1mO\u001b[0m|8|\n",
      "\n",
      "Enter your move from {5, 6, 8}:  5\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|\u001b[1mO\u001b[0m|\n",
      "|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|\u001b[1mO\u001b[0m|\n",
      "|\u001b[1mO\u001b[0m|\u001b[1mO\u001b[0m|8|\n",
      "\n",
      "Enter your move from {8}:  8\n",
      "\n",
      "Congrats! you win the game.\n",
      "\n",
      "______________________________\n",
      "\n",
      "\n",
      "Do you want to play again? (y/n) :y\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|2|7|6|\n",
      "|9|5|1|\n",
      "|4|3|8|\n",
      "\n",
      "Enter your move from {1, 2, 3, 4, 5, 6, 7, 8, 9}:  8\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|2|7|6|\n",
      "|9|5|\u001b[1mO\u001b[0m|\n",
      "|4|3|\u001b[1mX\u001b[0m|\n",
      "\n",
      "Enter your move from {2, 3, 4, 5, 6, 7, 9}:  5\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mO\u001b[0m|7|6|\n",
      "|9|\u001b[1mX\u001b[0m|\u001b[1mO\u001b[0m|\n",
      "|4|3|\u001b[1mX\u001b[0m|\n",
      "\n",
      "Enter your move from {3, 4, 6, 7, 9}:  3\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mO\u001b[0m|7|6|\n",
      "|9|\u001b[1mX\u001b[0m|\u001b[1mO\u001b[0m|\n",
      "|\u001b[1mO\u001b[0m|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|\n",
      "\n",
      "Enter your move from {6, 7, 9}:  6\n",
      "______________________________\n",
      "\n",
      "current board with available positions in\n",
      "\n",
      "|\u001b[1mO\u001b[0m|\u001b[1mO\u001b[0m|\u001b[1mX\u001b[0m|\n",
      "|9|\u001b[1mX\u001b[0m|\u001b[1mO\u001b[0m|\n",
      "|\u001b[1mO\u001b[0m|\u001b[1mX\u001b[0m|\u001b[1mX\u001b[0m|\n",
      "\n",
      "Enter your move from {9}:  9\n",
      "it's draw.\n",
      "\n",
      "______________________________\n",
      "\n",
      "\n",
      "Do you want to play again? (y/n) :n\n"
     ]
    }
   ],
   "source": [
    "my_policy = QPolicy()\n",
    "my_policy.set_q_table(q_table)\n",
    "experiment(my_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The model works perfect against random selection, however, it lacks reasoning against humans. For improving accuracy we have to optimize the Q-table we designed earlier. To do so, we need high computation time. Another method is to train a neural netwok which can outperform the previous models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
